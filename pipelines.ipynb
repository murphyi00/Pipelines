{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyOpGTED2IiytyIRft4T+kAa"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"5VSgDw0zJxJl"},"outputs":[],"source":["!pip install yfinance minio py2neo neo4j\n","!sudo cp /home/jovyan/work/jars/neo4j-connector-apache-spark_2.12-4.1.0_for_spark_3.jar /usr/local/spark/jars/\n","\n","import yfinance as yf\n","from pyspark.sql import SparkSession, Row\n","from pyspark.ml.feature import VectorAssembler\n","from pyspark.ml.stat import Correlation\n","from pyspark.sql.functions import col\n","from minio import Minio\n","from minio.error import S3Error\n","from py2neo import Graph, Node, Relationship, Subgraph\n","\n","# Minio configuration\n","s3_host = \"minio\"\n","s3_url = f\"http://{s3_host}:9000\"\n","s3_key = \"minio\"\n","s3_secret = \"SU2orange!\"\n","s3_bucket = \"minio-project\"\n","\n","# Initialize Minio\n","minio_client = Minio(\n","    f\"{s3_host}:9000\",\n","    access_key=s3_key,\n","    secret_key=s3_secret,\n","    secure=False\n",")\n","\n","# Create bucket with try block\n","try:\n","    if not minio_client.bucket_exists(s3_bucket):\n","        minio_client.make_bucket(s3_bucket)\n","        print(f\"Bucket '{s3_bucket}' created.\")\n","    else:\n","        print(f\"Bucket '{s3_bucket}' already exists.\")\n","except S3Error as err:\n","    print(f\"Minio error: {err}\")\n","\n","# Initialize Spark session with Minio and Neo4j configuration\n","spark = SparkSession.builder \\\n","    .master(\"local[*]\") \\\n","    .appName(\"yfinance-minio-spark\") \\\n","    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.1.2\") \\\n","    .config(\"spark.hadoop.fs.s3a.endpoint\", s3_url) \\\n","    .config(\"spark.hadoop.fs.s3a.access.key\", s3_key) \\\n","    .config(\"spark.hadoop.fs.s3a.secret.key\", s3_secret) \\\n","    .config(\"spark.hadoop.fs.s3a.path.style.access\", True) \\\n","    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n","    .config(\"spark.jars\", \"/usr/local/spark/jars/neo4j-connector-apache-spark_2.12-4.1.0_for_spark_3.jar\") \\\n","    .config(\"spark.neo4j.bolt.url\", \"bolt://neo4j:7687\") \\\n","    .config(\"spark.neo4j.bolt.user\", \"neo4j\") \\\n","    .config(\"spark.neo4j.bolt.password\", \"SU2orange!\") \\\n","    .getOrCreate()\n","\n","sc = spark.sparkContext\n","sc.setLogLevel(\"ERROR\")\n","\n","# Define a list of stocks to analyze\n","tickers = [\"AAPL\", \"AMZN\", \"MSFT\", \"GOOGL\", \"TSLA\", \"BRK-B\", \"JNJ\", \"V\", \"WMT\",\n","           \"PG\", \"UNH\", \"NVDA\", \"HD\", \"DIS\", \"PYPL\", \"MA\", \"VZ\", \"ADBE\", \"NFLX\",\n","           \"INTC\", \"CSCO\", \"KO\", \"PEP\", \"T\", \"PFE\", \"MRK\", \"ABT\", \"XOM\", \"CVX\",\n","           \"CRM\", \"MCD\", \"BMY\", \"NKE\", \"BA\", \"WBA\", \"IBM\", \"MMM\", \"GE\", \"AGG\",\n","           \"BIV\", \"BND\", \"BSV\", \"DIA\", \"EEM\", \"EFA\", \"ENZL\", \"EPHE\", \"EWA\",\n","           \"EWC\", \"EWG\", \"EWH\", \"EWI\", \"EWJ\", \"EWK\", \"EWM\", \"EWN\", \"EWQ\", \"EWS\",\n","           \"EWT\", \"EWU\", \"EWY\", \"EWZ\", \"EZU\", \"FEZ\", \"FLRN\", \"FXI\", \"GXC\", \"HYG\",\n","           \"IEF\", \"IJH\", \"IJR\", \"INDA\", \"ITOT\", \"IWB\", \"IWD\", \"IWF\", \"IWM\", \"IWO\",\n","           \"LQD\", \"MBB\", \"MCHI\", \"MUB\", \"OIH\", \"PFF\", \"QQQ\", \"REM\", \"RSX\", \"SHV\",\n","           \"SHY\", \"SLV\", \"SPY\", \"THD\", \"TLH\", \"TLT\", \"TUR\", \"VB\", \"VBK\", \"VBR\",\n","           \"VEA\", \"VGK\", \"VGT\", \"VO\", \"VOE\", \"VOO\", \"VPL\", \"VTI\", \"VTWO\", \"VUG\",\n","           \"VWO\", \"XLB\", \"XLE\", \"XLF\", \"XLI\", \"XLK\", \"XLP\", \"XLU\", \"XLV\", \"XLY\"]\n","\n","# Download stock data using yfinance\n","data = yf.download(tickers, start=\"2024-05-01\")\n","\n","# Flatten the column names and reset index\n","data.columns = ['_'.join(filter(None, col)).strip() for col in data.columns.values]\n","data.reset_index(inplace=True)\n","\n","# Convert to Spark DataFrame\n","spark_df = spark.createDataFrame(data)\n","\n","# Convert Date column to string\n","spark_df = spark_df.withColumn(\"Date\", col(\"Date\").cast(\"string\"))\n","\n","# Save the Spark DataFrame to Minio as CSV\n","output_path = f\"s3a://{s3_bucket}/stocks_data.csv\"\n","spark_df.write.mode(\"overwrite\").csv(output_path, header=True)\n","\n","print(f\"Data saved to Minio at {output_path}\")\n","\n","#\n","#\n","# Perform Spark operations\n","selected_columns = [col for col in spark_df.columns if col.startswith(\"Close_\")]\n","\n","# Convert columns to vector column first, using MLlib.  First bottleneck avoided here\n","vector_col = \"corr_features\"\n","assembler = VectorAssembler(inputCols=selected_columns, outputCol=vector_col, handleInvalid=\"skip\")\n","df_vector = assembler.transform(spark_df).select(vector_col)\n","\n","# Compute the correlation matrix\n","correlation_matrix = Correlation.corr(df_vector, vector_col).head()[0]\n","\n","# Convert correlation matrix to a readable format\n","correlation_df = spark.createDataFrame(\n","    [(float(correlation_matrix[i, j]), selected_columns[i], selected_columns[j])\n","     for i in range(len(selected_columns)) for j in range(i + 1, len(selected_columns))],\n","    [\"correlation\", \"stock1\", \"stock2\"]\n",")\n","\n","from pyspark.sql.functions import abs as spark_abs\n","\n","# Filter correlations based on a threshold\n","correlation_threshold = 0.5\n","filtered_correlations_df = correlation_df.filter(spark_abs(col(\"correlation\")) >= correlation_threshold)\n","\n","# Show the filtered correlations\n","filtered_correlations_df.show()\n","\n","#\n","#\n","# Save the filtered correlations to Neo4j\n","neo4j_url = \"neo4j://neo4j:7687\"\n","neo4j_user = \"neo4j\"\n","neo4j_password = \"SU2orange!\"\n","\n","# Initialize Neo4j connection\n","graph = Graph(neo4j_url, auth=(neo4j_user, neo4j_password))\n","\n","# Batch operations in Neo4j made it quicker, this fixed the second bottleneck\n","nodes = []\n","relationships = []\n","for row in filtered_correlations_df.collect():\n","    stock1 = row['stock1'].replace(\"Close_\", \"\")\n","    stock2 = row['stock2'].replace(\"Close_\", \"\")\n","    correlation = row['correlation']\n","\n","    # Create nodes and relationships in Neo4j\n","    ticker1_node = Node(\"Stock\", name=stock1)\n","    ticker2_node = Node(\"Stock\", name=stock2)\n","    correlation_rel = Relationship(ticker1_node, \"CORRELATES_WITH\", ticker2_node, correlation=correlation)\n","\n","    nodes.append(ticker1_node)\n","    nodes.append(ticker2_node)\n","    relationships.append(correlation_rel)\n","\n","# Create subgraph and merge it in one batch\n","subgraph = Subgraph(nodes, relationships)\n","graph.create(subgraph)\n","\n","print(\"Correlation results saved to Neo4j.\")\n","\n","# Cypher query to read data from Neo4j\n","cypher_query = \"\"\"\n","MATCH (s1:Stock)-[r:CORRELATES_WITH]->(s2:Stock)\n","RETURN s1.name AS stock1, s2.name AS stock2, r.correlation AS correlation\n","\"\"\"\n","\n","# Read data from Neo4j into a Spark DataFrame\n","df = spark.read.format(\"org.neo4j.spark.DataSource\") \\\n","    .option(\"url\", \"bolt://neo4j:7687\") \\\n","    .option(\"query\", cypher_query) \\\n","    .load()\n","\n","# Show the DataFrame\n","df.show()\n","\n","# Perform some logic: Filter for strong correlations\n","strong_correlations_df = df.filter(col(\"correlation\") > 0.8)\n","\n","# Show the filtered DataFrame\n","strong_correlations_df.show()\n","\n","# Save the filtered DataFrame back to Minio??\n","output_path = f\"s3a://{s3_bucket}/strong_correlations.csv\"\n","strong_correlations_df.write.mode(\"overwrite\").csv(output_path, header=True)\n","print(f\"Strong correlations saved to Minio at {output_path}\")\n","\n","# Writing back to Neo4j with stronger relationships\n","nodes = []\n","relationships = []\n","for row in strong_correlations_df.collect():\n","    stock1 = row['stock1']\n","    stock2 = row['stock2']\n","    correlation = row['correlation']\n","\n","    # Create nodes and relationships in Neo4j\n","    ticker1_node = Node(\"Stock\", name=stock1)\n","    ticker2_node = Node(\"Stock\", name=stock2)\n","    correlation_rel = Relationship(ticker1_node, \"STRONGLY_CORRELATES_WITH\", ticker2_node, correlation=correlation)\n","\n","    nodes.append(ticker1_node)\n","    nodes.append(ticker2_node)\n","    relationships.append(correlation_rel)\n","\n","# Create subgraph and merge it in one batch\n","subgraph = Subgraph(nodes, relationships)\n","graph.create(subgraph)\n","\n","print(\"Strong correlations saved to Neo4j.\")\n"]}]}